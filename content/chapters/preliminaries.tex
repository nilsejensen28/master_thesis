\setchapterpreamble[u]{\margintoc}
\chapter{Preliminaries}

This chapter aims to establish the framework used throughout this thesis to study the \Tarski\ problem. It formally introduces the concept of total search problems and the complexity class \TFNP, along with its subclasses \PLS, \PPAD, and \EOPL. In addition, this chapter will describe how functions and sets are represented in this framework, and how their complexity is measured. Finally, a formal introduction to the Tarski problem is given.

\section{Total search problems}

The study of complexity classes has traditionally focused on \define{decision problems}, which involve determining whether an object belongs to a set, also called a \define{language}. Notable examples include determining whether a Boolean formula is satisfiable, or whether a $k$-clique exists in a given graph. However, real-world questions often require explicit answers, and not simply existence results. For example, while deciding whether a function has a global minimum is a decision problem, the practical interest lies in actually identifying that minimum, which goes beyond mere existence.

This is where so called \define{search problems} come into play.

\subsection{Search problems}

\begin{definition}[Search Problem]
    \boxmarginnote{The ``No'' case can be encoded as some special binary string.}
    A \define{search problem} is given by a relation $R\subset \binstr \times \binstr$. For a given \define{instance} $I\in \binstr$ the computational problem, to find a \define{solution} $s \in \binstr$, that satisfies: $(I, s) \in R$ or output ``No'' if no such $s$ exists.
\end{definition}

We can view these search problems as decision problems by looking at the corresponding decision problem given by the language:
\marginnote{Here we have simply rephrased the valid language to be the pair of a problem instance and a valid solutions.}
\begin{align*}
    \mathcal{L}_R = \{ I \in \binstr |\ \exists s \in \binstr : (I, s) \in R\}
\end{align*}

This leads us to ask classical complexity questions about search problems: Are these problems in \P? in \NP? Are they \NP-hard? It is readily apparent that search problems are inherently at least as complex as their decision counterparts, since solving a search problem inherently solves the associated decision question. This observation leads to an intriguing question: what if the decision component is removed from the equation? This scenario can be achieved by ensuring that ``no'' is never a valid solution. Such problems, where every instance is garanteed to admit a solution, are called total search problems.

\begin{definition}[Total search problems]
    A \define{total search problem} is a search problem given by a relations $R\subset \binstr \times \binstr$, such that for every given instance $I\in
        \binstr$ there is a solution $s \in \binstr$, that satisfies: $(I, s) \in R$.
\end{definition}

The complexity class \TFNP\ as introduced in \sidecite{papadimitriou_complexity_1994} is simply the class of all total search problems that lie in \NP.\marginnote{This means that \TFNP\ can be seen as an intermediate class between \P\ and \NP, containing all search problems where a solution is guaranteed to exist, and where one can efficiently check the feasibility of a candidate solution.} Examples of \TFNP\ problems are:
\begin{itemize}
    \item \textsc{Factoring}, the problem of finding the prime factors of a number. Every number admits a factorisation into prime numbers, and this factorisation can be checked in polynomial time.
    \item \textsc{Nash}, the problem of finding a nash equilibrium in a bimatrix game;
    \item \textsc{Minimize}, the problem of finding the global minimum of a convex function.
\end{itemize}

\subsection{Reductions}

Similarly to decision problem we can also define reduction inside \TFNP.

\begin{definition}[Many-to-one Reduction]
    \boxmarginnote{Saying \emph{one can reduce $R$ onto $S$} can be understood as saying \emph{if one can solve $S$ efficiently then I can solve $R$ efficiently}.}
    For two problem $R, S \in \TFNP$, we say that $R$ \emph{reduces} (many to one) to $S$ if there exist polynomial time computable functions $f : \binstr \rightarrow \binstr$ and $g : \binstr \times \binstr \rightarrow \binstr$ such that for $I, s \in \binstr$: if $(f(I), s) \in S$ then $(I, g(I, s)) \in R$. This means that if $s$ is a solution to an instance $f(I)$ in $S$, we can compute $g(I, s)$ a solution to an instance $I$ in $R$
\end{definition}

We also introduce the notion of Turing reduction in \TFNP, analogously to the classical Turing reduction.

\begin{definition}[Turing Reduction]
    For two problems $R, S \in \TFNP$, we say that $R$ \emph{Turing reduces} to $S$ if polynomial-time oracle Turing machine that solves $R$ given access to an oracle for $S$ exists.
\end{definition}

\subsection{Promise Problems}

TODO: Talk about promise problems.

\section{Representation of functions and sets}[Representation of objects]

As we will see the problems we will work with, are given by questions of the form ``find an $x \in S$ such that $f(x)$ has some property''. This means that we should describe how we represent the input, that is the set $S$ and the function $f$. We start by describing how we represent sets.

\subsection{Representation of sets}[Representing sets]

In this thesis we will work with sets of the form $S = \{0, \dots, 2^n - 1\}$, which we will denote by $[2^n - 1]$. Notice that this set can be identified with the set of binary strings of length $n$. We will denote the set of binary strings of length $n$ by $\bitstr^n$. Formally the functions, and the model we will use to represent the functions will use the underlying binary strings in $\bitstr^n$. For notational convenience we will often only denote the integer $x \in [2^n - 1]$ instead of the binary string.

Similary when considering the $d$-dimensional case, we can represent the set $L = [2^n - 1]^d$, which corresponds to a $d$-dimensional lattice with side length $2^n$, as the set of binary strings of length $n \cdot d$: $\bitstr^{nd}$. Again for simplicity while the underlying functions rely on the binary strings, we will often only denote the point $(x_1, \dots, x_d) \in [2^n - 1]^d$, instead of its binary representation.

\subsection{Representation of functions}[Representing functions]

Now that we have described how we describe the sets, we can describe how we represent the functions. We will represent the functions by using so-called boolean circuits. In this section we will rely on the presentation of boolean circuits as described in \sidecite{greenlaw_chapter_1998}, and refer an interested reader to this source for a more detailed description.

On a high level a boolean circuit is a directed acyclic graph, where the nodes are called \define{gates}, and the edges are called \define{wires}. The sinks of the graphs are the output gates, and the sources are the input gates. We want to start by defining a gate formally.

\begin{definition}[Gate]
    \marginnote[-10mm]{This corresponds to the gate node, having $k$ incoming edges, and one outgoing edge.}
    A gate is a function $g : \bitstr^k \rightarrow \bitstr$, where $k$ is the number of input wires of the gate.
\end{definition}

In this thesis we will only consider the following types of gates:
\begin{itemize}
    \marginnote{Notice that we only consider gates with at most two inputs, as we can always represent a gate with $k$ inputs as a composition of gates with at most two inputs.}
    \item \textbf{AND-gate}: $g(x_1, x_2) = x_1 \land x_2$,
    \item \textbf{OR-gate}: $g(x_1, x_2) = x_1 \lor x_2$,
    \item \textbf{NOT-gate}: $g(x) = \lnot x$.
\end{itemize}

Now we can describe a boolean circuit, formally as follows:
\begin{definition}[Boolean circuit]
    A boolean circuit $C$ is a labeled finute directed acyclic graph, where each vertex has a \define{type} $\tau$, with
    \begin{align*}
        \tau(v) \in \{\text{INPUT}\} \cup \{\text{OUTPUT}\} \cup \{\text{AND}, \text{OR}, \text{NOT}\}
    \end{align*}
    and with the follwing properties:
    \begin{itemize}
        \item If $\tau(v) = \text{INPUT}$, then $v$ has no incoming edges. We call these vertices the \define{inputs gates}.
        \item If $\tau(v) = \text{OUTPUT}$, then $v$ has one incoming edge. We call these vertices the \define{output gates}.
        \item If $\tau(v) = \text{AND}$, then $v$ has two incoming edges. We call these vertices the \define{AND-gates}.
        \item If $\tau(v) = \text{OR}$, then $v$ has two incoming edges. We call these vertices the \define{OR-gates}.
        \item If $\tau(v) = \text{NOT}$, then $v$ has one incoming edge. We call these vertices the \define{NOT-gates}.
    \end{itemize}
    The inputs of $C$ are given by a tuple $(x_1, \dots, x_k)$, of distinct input gates. The output of $C$ is given by a tuple $(y_1, \dots, y_l)$ of distinct output gates.
\end{definition}

We give an example of such a boolean circuit in \cref{fig:boolean_circuit_example}. Of course we now want to use a boolean circuit to represent a function. In order to do this we need give a formal definition of the function computed by a boolean circuit.

\begin{figure}
    \centering
    \incfig{Boolean_Circuit_Example}
    \caption[Example of a Boolean Circuit]{Example of a boolean circuit with three input gates and four output gates.}
    \label{fig:boolean_circuit_example}
\end{figure}

\begin{definition}[Computed function of a boolean circuit]
    A boolean circuit $C$ with inputs $x_1, \dots, x_n$ and outputs $y_1, \dots, y_m$ computes a function $f : \bitstr^n \rightarrow \bitstr^m$ as follows:
    \begin{itemize}
        \item The input $x_i$ is assigned the value of the $i$-th bit of the argument to the function.
        \item Every other vertex $v$ is assigned the value of the gate $g$ of the vertex, applied to the values of the incoming edges of $v$.
        \item The $i$-th bit of the output of the function is the value of the output gate $y_i$.
    \end{itemize}
\end{definition}

\begin{figure}
    \centering
    \incfig{Computing_Function_Example}
    \caption[Computing a function with circuits]{Example of how a function $f : \{0, 1\}^2 \rightarrow \{0, 1\}^2$ (on the top), can be computed using boolean circuits (on the bottom).}
    \label{fig:computing_function_example}
\end{figure}

In \cref{fig:computing_function_example} we give an example of using a boolean circuit to compute a function, in particular for a function which is a \Tarski\ instance. From now on all functions used in problems will be formally represented by boolean circuits.

\subsection{Complexity of boolean circuits}[Complexity of circuits]

Of course formally the complexity of a problem is defined in terms of the \emph{size} of the input. This means that we also need to define what we mean by the size of a boolean circuit. We will use the following definition:

\begin{definition}[Size of a boolean circuit]
    The size of a boolean circuit $C$ is the number of gates in the circuit.
\end{definition}

Size is a measure of the complexity of the input, i.e.\ it gives us an indication of how many bits we need to represent the input\marginnote{It can be shown that $\poly(size(n))$ bits suffice to encore to encode any boolean circuit.}, it also tells us how many computations are made when computing the function output. We also define the depth of a boolean circuit, as follows:

\begin{definition}[Depth of a boolean circuit]
    The depth of a boolean circuit $C$ is the length of the longest path from an input gate to an output gate.
\end{definition}

The depth of a boolean circuit is a measure of the time complexity of the computation, i.e.\ it tells us how many time steps are needed to compute the output of the function. This is espacially true in a parallel setting, where all gates can be seen as setting off at the same time (exactly as in a CPU).

\section{Subclasses of \TFNP}

The existence of complete \FNP\ problems within \TFNP\ would imply that $\NP = \coNP$ \sidecite{megiddo_total_1991}, a scenario considered highly unlikely. Consequently, complete problems are not expected within \TFNP, necessitating alternative approaches to investigate its structure.

\TFNP\ is characterized as a \define{semantic} class, which means that it is difficult to verify whether a Turing machine defines a language within this class. In contrast, \define{syntactic} classes such as \P\ and \NP\ are characterized by the ease with which one can confirm that a Turing machine's accepted language belongs to the class. We refer the reader to Papadimitriou's work \sidecite{papadimitriou_computational_1994} for a more detailed discussion of these terms.

To address these challenges, we want to explore syntactic subclasses of \TFNP. One approach, proposed by Papadimitriou \cite{papadimitriou_computational_1994}, is to categorize search problems based on existence proofs that confirm their totalness. This basic strategy leads to the detailed study of specific complexity classes discussed in the following sections.

\subsection{Polynomial Local Search (\PLS)}[\PLS]

The existence results which gives rise to \PLS\ is:
\principle{Every directed acyclic graph has a sink.}
We can then construct the class \PLS\ by defining it as all problems which reduce to finding the sink of a directed acyclic graph (DAG). Formally we first define the problem \Localopt\ as in \sidecite{johnson_how_1988}:

\problem{Localopt}{
\boxmarginnote{$S$ can be seen as a proposed successor, and $V$ as a potential. The goal is to find a local minima $v$ of the potential.}
Two boolean circuits $S, V : [2^n] \rightarrow\ [2^n]$.}{A vertex $v \in [2^n]$ such that $P(S(v)) \geq\ P(v)$.}

\begin{figure}
    \centering
    \incfig{LOCALOPT_Solution_Example}
    \caption[Example of a \Localopt\ Problem]{Example of a \Localopt\ Problem with $n=3$ (8 vertices). The circuit $S$ is represented by solid lines. The valid solutions are colored green.}
    \label{fig:localopt_example}
\end{figure}

One might ask why this is equivalent to finding the sink of a DAG\@? The circuit $S$ defines a directed graph, which might contain cycles. Only keeping the edges on which the potential decreases (strictly) leads to a DAG, with as sinks exactly the $v$ such that $P(S(v)) \geq\ P(v)$. We give an example of a \Localopt\ instance in \cref{fig:localopt_example}. Now we can define \PLS:

\begin{definition}[Polynomial Local Search (\PLS)]
    The class \PLS\ is the set of all \TFNP\ problems that reduce to \Localopt.
\end{definition}

Studying ``easy'' problems such as PLS is particularly insightful because we strongly believe that these problems cannot be solved by any method more efficient than simply traversing the graph.\marginnote[-15mm]{By ``easy'' we mean that the problem can be solved by simply walking through the graph, and checking whether every vertex is a local minima.}. Hence given a graph of exponentially large size, it appears highly improbable to find an efficient solution. Therefore, all problems in \PLS\ inherently embody the fundamental challenge of not being able to surpass the basic strategy of navigating through the a directed acyclic graph. Of course --- and here lies the difficulty of complexity theory --- we cannot prove this statement, it could be that some very clever analysis of the boolean circuits, could lead to an efficient algorithm for finding sinks of exponentially large directed acyclic graphs.

\subsection{Polynomial Parity Argument on Directed Graphs (\PPAD)}[\PPAD]

Now we want to discuss the complexity class \PPAD, introduced by Papadimitriou as one of the first syntatic subclasses of \TFNP\ in~\sidecite{papadimitriou_complexity_1994}. The existence result giving rise to this class is: \principle{If a directed graph has an unbalanced vertex, then it has at least one other unbalanced vertex.}
\PPAD\ can be defined using the problem \textsc{End-of-Line} as introduced in \sidecite{daskalakis_complexity_2009}.

\problem{End-of-Line}{
Boolean circuit $S, P : \bitstr^n \rightarrow \bitstr^n$ such that $P(0^n) = 0^n \neq S(0^n)$ ($0^n$ is a source.)
}{
An $x \in \bitstr^n$ such that either:
\begin{itemize}
    \item $P(S(x)) \neq x$ ($x$ is a sink) or
    \item $S(P(x)) \neq x \neq 0^n$ ($x$ is a non non-standard source)
\end{itemize}
}
\marginnote[-20mm]{Here $S$ can be thought of giving the successor of a vertex, and $P$ as giving the predecessor of a vertex.}

\begin{figure}[ht]
    \centering
    \incfig{PPAD_Example}
    \caption[Example of an \textsc{End-of-Line} Problem]{Example of an \textsc{End-of-Line} Problem with $n=3$ (8 vertices).
        The circuit $S$ is represented by solid lines and the circuit $P$ by dashed lines.
        The solutions are the sinks $x=5$, $x=7$ and $x=1$, aswell as the sources $x=4$ and $x=6$.}
    \label{fig:ppad_example}
\end{figure}

These boolean circuits represent a directed graph with maximal in and out degree 1, by having an edge from $x$ to $y$ if and only if $S(x) = y$  and $P(y) =
    x$.
The goal is to find a sink of the graph, or another source.
\marginnote{Notice that \textsc{End-of-Line} allows cycles, and that these do not induce solutions.}
It can be shown that the general case of finding a second imbalanced vertex in a directed graph (a problem called \textsc{Imbalance}) can be reduced to
\textsc{End-of-Line} \sidecite{goldberg_hairy_2021}.
Now we can define the complexity class \PPAD\ as follows:

\begin{definition}[\PPAD]
    The class \PPAD\ is the set of all \TFNP\ problems that reduce to \textsc{End-of-Line}.
\end{definition}

\subsection{End of Potential Line (\EOPL)}[\EOPL]

Next we want to discuss the complexity class \EOPL{} as introduced in \sidecite{EOPL_introduction}. The existence results giving rise to \EOPL\ is:
\principle{In a directed acyclic graph, there must be at least two unbalanced vertices.}
Similarly to \PLS\ acyclicity will be enforced using a potential.

\problem{End of Potential Line}{
Two boolean circuits $S, P : \bitstr^n \rightarrow \bitstr^n$, and a boolean circuit $V : \bitstr^n \rightarrow [2^n - 1]$, such that $0^n$ is a source, (i.e.\
$P(0^n) = 0^n \neq S(0^n)$).
}{
An $x \in \bitstr^n$ such that either:
\begin{itemize}
    \item $P(S(x)) \neq x$ ($x$ is a sink)
    \item $S(P(x)) \neq x \neq 0^n$ ($x$ is a \define{non-standard source})
    \item $S(x) \neq x$, $P(S(x)) = x$ and $V(S(x)) \leq V(x)$ (violation of the monoticity of the potential)
\end{itemize}
}
\marginnote[-50mm]{Here $S$ can be thought of giving the successor of a vertex, and $P$ as giving the predecessor of a vertex.
    $V$ can be though of as a potential which is suppose to be monotonously increasing along the line.}

\begin{figure}[ht]
    \centering
    \incfig{EOPL_Solution_Example}
    \caption[Example of an \EOPL\ Problem]{Example of an \EOPL\ Problem with $n=3$ (8 vertices).
        The circuit $S$ is represented by solid lines and the circuit $P$ by dashed lines.
        The solutions are the sink $x=7$, the violation of potential at $x=5$ and the non-standard source $x=4$.}
    \label{fig:eopl_example}
\end{figure}

$S$ and $P$ can be though of as representing a directed line. Finding another source (a non-standard source), is a violation, as a directed line only has one source. The potential serves a garantee of acyclicity. Now we can define the complexity class \EOPL.

\begin{definition}[\EOPL]
    The class \EOPL\ is the set of all \TFNP\ problems that reduce to \textsc{End of Potential Line}.
\end{definition}

\section{The \Tarski\ Problem}[\Tarski\ Problem]
\label{sec:tarski_problem}

\subsection{Definition of the \Tarski\ Problem}[\Tarski\ Definition]

Next we want to introduce the \Tarski\ Problem. Before we do this we recall that there is a partial order on the $d$ dimensional latice ${[N]}^d$, given by $x \leq y$ iff $x_i \leq y_i$ for all $i \in \{1, \dots, d\}$\marginnote{Notice that $x \not\leq y$ does \emph{not} imply $x \geq y$. In particular two points are not always comparable.}. We can now define functions on this lattice, and in particular we can define monotone functions.

\begin{definition}[Monotone function]
    \boxmarginnote{Such functions are also called \define{order preserving} functions in the litterature.}
    A function $f : {[N]}^d \rightarrow {[N]}^d$ is \define{monotone} if for all $x, y \in {[N]}^d$ we have $x \leq y$ implies $f(x) \leq f(y)$.
\end{definition}

The name of the \Tarski\ problem originates from Tarski's fixed point Theorem, first introduced in \sidecite{tarski_lattice-theoretical_1955} which we remind the reader of below:

\begin{theorem}[Tarski's fixed point Theorem]
    \boxmarginnote{This theorem is also known as the Knasterâ€“Tarski Theorem in the litterature.}
    Let $f : {[N]}^d \rightarrow {[N]}^d$ a function on the $d$-dimentional lattice. If $f$ is monotonous (with respect to the previously discussed partial order), then $f$ has a fixed point, i.e.\ there is an $x \in {[N]}^d$ such that $f(x)=x$.
\end{theorem}

A proof of this theorem can be found in the previously mentionned work~\cite{tarski_lattice-theoretical_1955}. Without surprise the \Tarski\ problem as defined in \sidecite{etessami_tarskis_2020}, is now to find such a fixed-point. Formally we define the problem as follows:
\problem{Tarski}{A boolean circuit $f : {[N]}^d \rightarrow {[N]}^d$.}{Either:
\begin{itemize}
    \item An $x \in {[N]}^d$ such that $f(x)=x$ (fixed point) or
    \item $x, y \in {[N]}^d$ such that $x \leq y$ and $f(x) \nleq f(y)$ (violation of monoticity).
\end{itemize}
}

\begin{figure}
    \centering
    \incfig{Tarski_Solution_Example}
    \caption[Example of a \Tarski\ instance]{Example of a 2 dimensional \Tarski\ instance.}
    \label{fig:tarski_example}
\end{figure}

This is of course a total search problem, as there will always either be a fixed point, or a point violating monoticity. We give an example of a 2 dimensional \Tarski\ instance in \cref{fig:tarski_example}. Before we discuss the location of \Tarski\ in the \TFNP\ lanscape and two known algorithms for solving \Tarski, we want to discuss a useful Lemma, which allows us to simplify the study of Tarski instances. The definition of \Tarski\ instaSnces allows for the image of a point to be located anywhere in the lattice, we will show that we can reduce to the cases where the image of a point is in the immediate neighborhood of the point.

\begin{lemma}[Simplyfying \Tarski]
    Let $f : L \rightarrow L$ be a \Tarski\ instance on a complete lattice $L$. Consider $\ftilde : L \rightarrow L$ given by: \boxmarginnote{Notice that given a circuit $C$ which computes $f$, we can construct a circuit $\Ctilde$ which computes $\ftilde$ by adding $\BigO{d}$ gates to $C$. This means that both problems are equivalent in terms of complexity.}
    \begin{align*}
        \ftilde(x)[i] = \begin{cases}
                            x[i] + 1 & \text{ if } f(x)[i] > x[i], \\
                            x[i]     & \text{ if } f(x)[i] = x[i], \\
                            x[i] - 1 & \text{ if } f(x)[i] < x[i].
                        \end{cases} \quad \text{for all $i \in \set{1, \dots, d}$}
    \end{align*}
    Then for any two points $x, y \in L$, $f(x) \leq f(y)$ if and only if $\ftilde(x) \leq \ftilde(y)$.
\end{lemma}
\begin{proof}
    The lemma follows directly by observing that for all $i \in \set{1, \dots, d}$ we have: $f(x)[i] \leq f(y)[i]$ if and only if $\ftilde(x)[i] \leq \ftilde(y)[i]$.
\end{proof}
This means that in the whole thesis we can consider the simplified version of the \Tarski\ problem, where for every $x \in L$ we have $\norminf{x - f(x)} \leq 1$, which we will implicitely assume from now on.

\subsection{Two algorithms for solving \Tarski}[\Tarski\ Algorithms]

We briefly want to discuss the most commun algorithms used for solving \Tarski\ instances. We begin with a very simple algorithm, which is based on the following observation:
\begin{remark}
    Let $f$ be a \Tarski\ instance on a complete lattice $L$. If $f$ is monotonous, and for some $x \in L$ we have $f(x) \geq x$, then $f(f(x)) \geq f(x)$.
\end{remark}
Now by noticing that by starting with the point $0^d$, and iterating the function $f$ we will eventually reach a fixed point, we can construct an iterative Algorithm for solving \Tarski, described in \cref{alg:iterative_tarski_solver}.

\begin{algorithm}
    \caption{Iterative Algorithm for \Tarski}
    \label{alg:iterative_tarski_solver}
    \KwData{A boolean circuit $f : L \rightarrow L$}
    \KwResult{A fixed point of $f$}
    $x \leftarrow 0^d$ \;
    \While{$f(x) \neq x$}{
        $x \leftarrow f(x)$ \;
    }
    \Return $x$ \;
\end{algorithm}

While \cref{alg:iterative_tarski_solver} might not be very efficient --- it runs in worst-case time $\BigO{d \cdot N}$ for $L = [N]^d$ --- it does have some theoretical applications for locating \Tarski\ inside \TFNP. Previous work \sidecite{etessami_tarskis_2020} showed that \Tarski\ lies in \PLS, by considering the set of possible states of the previously described algorithm, together with a potential function given by $V(x) = \sum_{i=1}^{d}{x[i]}$, and showing that this potential is monotonous along the the states of the algorithm. The circuit $S$ associates to state of the algorithm the next state it will be in.

Next we want to describe a more advanced algorithm for solving \Tarski\ instances. The algorithm we will present is due to \sidecite{dang_computations_2020}. We will give an alternative presentation and simplified proof the correctness of the algorithm here. Before we do this we want to introduce some notation in order to make the argument as clear as possible. For a given complete lattice $L = [N_1] \times \cdots \times [N_d]$ and some dimension $x \in L$ we define the following sublattices:
\begin{align*}
    L_{\leq x} & = [x[1]] \times \cdots \times [x[d]],                             \\
    L_{\geq x} & = \winterval{x[1]}{N_1} \times \cdot \times \winterval{x[d]}{N_d}
\end{align*}
and for a given dimension $k \in \set{1, \dots, d}$ and $K \in [N_k]$, we define the following sublattices:
\begin{align*}
    L_{k < K} & = [N_1] \times \cdots \times [N_{k-1}] \times [K-1] \times [N_{k+1}] \times \cdots \times [N_d],               \\
    L_{k = K} & = [N_1] \times \cdots \times [N_{k-1}] \times \{K\} \times [N_{k+1}] \times \cdots \times [N_d],               \\
    L_{k > K} & = [N_1] \times \cdots \times [N_{k-1}] \times \{K+1, \dots, N_k\} \times [N_{k+1}] \times \cdots \times [N_d].
\end{align*}
The algorithm --- and in particular our proof of the correctness --- is based on the following observation:
\begin{remark}
    Let $L = [N_1] \times \cdots \times [N_d]$ be a complete lattice and $f : L \rightarrow L$ a monotonous function. Then:
    \begin{enumerate}
        \item If for some $x \in L$ we have $f(x) \leq x$, then $f$ has a fixed point in $L_{\leq x}$.
        \item If for some $x \in L$ we have $f(x) \geq x$, then $f$ has a fixed point in $L_{\geq x}$.
    \end{enumerate}
\end{remark}
\begin{proof}
    Let $x \in L$ such that $f(x) \leq x$. Then for all $y \in L_{\leq x}$ we have $y \leq x$ and hence $f(y) \leq f(x) \leq x$, which shows that $f$ is a \Tarski\ instance on $L_{\leq x}$. By Tarski's fixed point Theorem, $f$ has a fixed point in $L_{\leq x}$. The proof for the second point is analogous.
\end{proof}
Hence points with these propeties seem to be particularly interesting for finding fixed points of $f$. Hence we want to give them a name:
\begin{definition}[Progress point]
    \boxmarginnote{The smallest and largest point of a lattice are always progress points.}
    Let $f : L \rightarrow L$ a \Tarski\ function. We call a point $x \in L$ a \emph{progress point} if $f(x) \leq x$ or $f(x) \geq x$.
\end{definition}
This means that if we a progress point we can reduce the area in which we need to search for a fixed point. The question now becomes: how do we find such an $x$? The algorithm we will present is based on the following observation:
\begin{remark}
    Let $f : L \rightarrow L$ on a complete lattice ${L = [N_1] \times \cdots \times [N_d]}$, for a monotonous function $f$, be a \Tarski\ instance. By fixing some dimension ${k \in \set{1, \dots, d}}$, we can define the function ${f_{k=K} : L_{k=K} \rightarrow L_{K=k}}$ as follows:
    \begin{align*}
        f_{k=K}(x)[i] = \begin{cases}
                            f(x)[i] & \text{ if } i \neq k, \\
                            K       & \text{ if } i = k.
                        \end{cases} \quad \text{for all $i \in \set{1, \dots, d}$}
    \end{align*}
    Then $f_{k=K}$ is a monotone \Tarski\ instance on $L_{k=K}$, and if $x^*$ is a fixed point of $f_{k=K}$, then $x^*$ is a progess point of $f$.
\end{remark}
This means that if we can solve a $d-1$ dimensional \Tarski\ instance, we can find a point $x$ such that $f(x) \geq x$ or $f(x) \leq x$.
\begin{proof}
    The monotonicity of $f_{k=K}$ follows directly from the monoticity of $f$. \par
    The fact that $x^*$ is a progess point follows from the fact that if $x^*$ is a fixed point of $f_{k=K}$, then $f(x^*)[i] = x^*[i]$, for all $i \neq k$. This means that if $f(x^*)[k] \leq x^*[k]$, then $f(x^*) \leq x^*[k]$ and if $f(x^*)[k] \geq x^*[k]$, then $f(x^*) \geq x^*[k]$.
\end{proof}
By choosing $K = \lfloor \frac{N_k}{2} \rfloor$ we can find a progress point $x$ such that both $L_{\leq x}$, and $L_{\geq x}$ have at most half the size of $L$. This means that we can reduce the search space by a factor of at least 2, by solving a $d-1$ dimensional \Tarski\ instance. We can solve a $d$ dimensional \Tarski\ instance by repeatedly solving $d-1$ dimensional \Tarski\ instances, and reducing the size of the search space by a factor of at least 2 in each step. This means that we can solve a $d$ dimensional \Tarski\ instance with a combination of a $d-1$ dimensional \Tarski\ solver and a binary search. The $d-1$ dimensional instances can be solved recursively. We give the recursive algorithm for solving \Tarski\ instances in \cref{alg:recursive_tarski_solver}.
\begin{algorithm}
    \caption{Recursive Algorithm for \Tarski}
    \label{alg:recursive_tarski_solver}
    \SetKwFunction{RecursiveTarskiSolver}{RecursiveTarskiSolver}
    \SetKwProg{Fn}{Function}{:}{}
    \Fn{\RecursiveTarskiSolver{$f : L \rightarrow L$, $d$}}{
        \tcc{Binary search in the $d$-th dimension}
        \Let{l}{0}, \Let{r}{$N_d$} \tcc*[r]{The search space is $[l, r]$}
        \While{$r - l > 1$}{
            \Let{$m$}{$\floor{\frac{l + r}{2}}$} \tcc*[r]{Middle of the interval}
            \If{$d-1 = 0$}{
                \Let{$x^*$}{$m$}
            }
            \Else{
                \Let{$L'$}{$L_{d = m}$} \;
                \tcc{Solve the $d-1$ dimensional instance}
                \Let{$x^*$}{\RecursiveTarskiSolver{$f_{d=m}, d-1$}} \;
            }
            \If{$f(x^*)[d] \leq x^*[d]$}{
                \Let{r}{$m$}
            }
            \Else{
                \Let{l}{$m$}
            }
        }
        \Return{$x^*$}
    }
\end{algorithm}

A simple analysis shows that this algorithm runs in $\BigO{\log^{d}{N}}$ for $L = [N]^d$. It was conjectured by Etessami et.\ al.\ that this is an optimal algorithm for \Tarski\ \sidecite{etessami_tarskis_2020}. This turned out not to be true, as a better algorithms was developped, which mostly relies on a smarter way of finding progress points, or better said, a smarter way of dividing the problem in subinstances \sidecite{fearnley_faster_2022}. We will not discuss this algorithm in detail, as it is not relevant for the rest of the thesis, but want to mentions that it achieves a runtime of $\BigO{\log^{2\ceil{\frac{d}{3}}}{N}}$ for $L = [N]^d$. This is to date the best upper bound for solving \Tarski\ instances.

\subsection{Lower bounds for \Tarski}

We now want to discuss the lower bounds for solving \Tarski\ instances. The best known lower bounds for \Tarski\ are given by \sidecite{etessami_tarskis_2020}. They showed that in the black-box model, where the only way to access the function $f$ is by querying it, solving a $d$-dimensional \Tarski\ requieres solving at least $\Omega(\log^{N})$ one-dimensional \Tarski\ instances, which are as difficult as binary search,hence this means that solving a $d$-dimensional \Tarski\ instance requieres at least $\Omega(\log^{2}{N})$ queries. This means that the upper and lower bounds are equal in the 2 dimensional case, but in all other cases there remains a gap. In particular the best known lower bound for solving \Tarski\ does not depend on the dimension $d$, which seems somewhat unexpected.

This gives us reason to study \Tarski\ under the lense of complexity theory, in particular to understand where \Tarski\ lies in the \TFNP\ landscape.

\subsection{Location of \Tarski\ in \TFNP}[\Tarski\ in \TFNP]

We now want to summarize where \Tarski\ lies inside of \TFNP. It has been shown in~\cite{etessami_tarskis_2020} that \Tarski\ lies in \PLS\ as we discussed when presenting \cref{alg:iterative_tarski_solver}. The same paper showed that \Tarski\ lies $\P^{\PPAD}$. We will give an alternative proof of this second fact in \cref{ch:ppad_reduction}. Previous work~\sidecite{buss_propositional_2012}, showed that many-to-one reductions and Turing-reduction onto \PPAD\ are equivalent. In particular this means that $\P^{\PPAD} = \PPAD$, and that \Tarski\ lies in \PPAD{}.

Now that we have established that \Tarski\ lies inside $\PLS \medcap \PPAD$, we want to discuss the structure of $\PLS \medcap \PPAD$ and describe recent advances in the study of this class. There have been two suprising advances in the study of $\PLS \medcap \PPAD$ in the last years. The first is that $\CLS = \PLS \medcap \PPAD$ \sidecite{fearnley_complexity_2023}. \CLS\ (Continuous Local Search) was first introduced by Daskalakis and Papadimitriou in \sidecite{daskalakis_continuous_2011}, and can be informally thought of the class of all problems which can be solved by finding the local optimum of a potential in a discrete space with an adjacency relations. This result shows that the problems in $\PLS \medcap \PPAD$ are exactly those that can be solved by gradient descent algotithms.

A further notable collapse is the result $\PLS \medcap \PPAD = \EOPL$, which was only recently shown in \sidecite{goos_further_2022}. This of course means that in particular \Tarski\ lies in \EOPL. A question which then arise, and which this thesis will try to answer, is can we construct an explicit reduction of \Tarski\ to \EndOfPotentialLine.